import torch
from ultralytics.models import YOLO
import os
import numpy as np
from functools import wraps

def fix_plot_images_function():
    """ä¿®å¾© plot_images å‡½æ•¸çš„ cls åƒæ•¸å•é¡Œ"""
    try:
        import ultralytics.utils.plotting as plotting
        
        # ä¿å­˜åŸå§‹å‡½æ•¸
        original_plot_images = plotting.plot_images
        
        @wraps(original_plot_images)
        def fixed_plot_images(*args, **kwargs):
            # ç¢ºä¿åƒæ•¸é †åºæ­£ç¢ºï¼šimages, batch_idx, cls
            if len(args) < 3:
                # å¦‚æœåƒæ•¸ä¸è¶³ï¼Œå˜—è©¦å¾è¨“ç·´ä¸Šä¸‹æ–‡ä¸­ç²å–æ­£ç¢ºçš„é¡åˆ¥ä¿¡æ¯
                args = list(args)
                
                # å˜—è©¦å¾é å®šç¾©é¡åˆ¥æ¨¡çµ„è®€å–é¡åˆ¥æ•¸é‡
                try:
                    import sys
                    from pathlib import Path
                    
                    # æ·»åŠ configç›®éŒ„åˆ°Pythonè·¯å¾‘
                    config_dir = Path(__file__).parent.parent / 'config'
                    if str(config_dir) not in sys.path:
                        sys.path.insert(0, str(config_dir))
                    
                    from predefined_classes import get_predefined_classes_count  # type: ignore
                    nc = get_predefined_classes_count()
                    print(f"ğŸ“Š å¾é å®šç¾©é¡åˆ¥æª¢æ¸¬åˆ°é¡åˆ¥æ•¸é‡: {nc}")
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•è®€å–é å®šç¾©é¡åˆ¥ï¼Œä½¿ç”¨é»˜èªå€¼: {e}")
                    nc = 1  # é»˜èª1å€‹é¡åˆ¥
            
            # èª¿ç”¨åŸå§‹å‡½æ•¸
            return original_plot_images(*args, **kwargs)
                
        
    except Exception as e:
        print(f"âš ï¸ plot_images å‡½æ•¸ä¿®å¾©å¤±æ•—: {e}")

def main(model_path, confidence_threshold=0.25, device=None, predict_data_dir=None):
    """
    ä¸»æ¨ç†å‡½æ•¸ - åŸºæ–¼test.pyçš„æ”¹é€²ç‰ˆæœ¬
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾‘ (å¿…éœ€)
        confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼ï¼Œé»˜èª0.25
        device: è¨­å‚™é¡å‹ï¼Œå¦‚æœç‚ºNoneå‰‡è‡ªå‹•æª¢æ¸¬
        predict_data_dir: é æ¸¬æ•¸æ“šç›®éŒ„ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨é»˜èªç›®éŒ„
    """
    # ä¿®å¾© plot_images å‡½æ•¸
    fix_plot_images_function()
    
    # æª¢æŸ¥å¿…éœ€åƒæ•¸
    if not model_path:
        print("éŒ¯èª¤: å¿…é ˆæä¾›æ¨¡å‹æ–‡ä»¶è·¯å¾‘ / Error: Model path is required")
        return
    
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device} / Using device: {device}")

    # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"éŒ¯èª¤: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ / Error: Model file not found: {model_path}")
        return

    # è¼‰å…¥æ¨¡å‹
    print("è¼‰å…¥æ¨¡å‹ä¸­... / Loading model...")
    model = YOLO(model_path)
    print("æ¨¡å‹è¼‰å…¥æˆåŠŸ / Model loaded successfully")

    # è¨­ç½®é æ¸¬æ•¸æ“šç›®éŒ„
    if predict_data_dir is None:
        predict_data_dir = r"Predict\Data"
    
    if not os.path.exists(predict_data_dir):
        print(f"éŒ¯èª¤: é æ¸¬æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨ / Error: Predict data directory not found: {predict_data_dir}")
        return
    
    # æª¢æŸ¥æ˜¯å¦æœ‰åœ–ç‰‡æ–‡ä»¶
    image_files = []
    for ext in ['.npy', '.jpg', '.jpeg', '.png', '.bmp']:
        image_files.extend([f for f in os.listdir(predict_data_dir) if f.lower().endswith(ext)])
    
    if not image_files:
        print(f"éŒ¯èª¤: åœ¨ {predict_data_dir} ä¸­æœªæ‰¾åˆ°åœ–ç‰‡æ–‡ä»¶ / Error: No image files found in {predict_data_dir}")
        return
    
    print(f"æ‰¾åˆ° {len(image_files)} å€‹åœ–ç‰‡æ–‡ä»¶ / Found {len(image_files)} image files")

    # ç¢ºä¿çµæœç›®éŒ„å­˜åœ¨ - ä½¿ç”¨å¸¶æ™‚é–“æˆ³çš„runsç›®éŒ„
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%dT%H%M")
    result_dir = f"runs/yolo_inference/{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    
    # å°æ¯å¼µåœ–ç‰‡é€²è¡Œæ¨ç†
    results = []
    for i, image_file in enumerate(image_files):
        try:
            print(f"è™•ç†åœ–ç‰‡ {i+1}/{len(image_files)}: {image_file} / Processing image {i+1}/{len(image_files)}: {image_file}")
            
            # æ§‹å»ºå®Œæ•´è·¯å¾‘
            image_path = os.path.join(predict_data_dir, image_file)
            
            # å¦‚æœæ˜¯.npyæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨numpyæ•¸çµ„ / If it's a .npy file, use numpy array directly
            if image_file.lower().endswith('.npy'):
                # è¼‰å…¥numpyæ•¸çµ„ (ä¿æŒfloat16æ ¼å¼) / Load numpy array (keep float16 format)
                img_array = np.load(image_path)
                print(f"è¼‰å…¥numpyæ•¸çµ„ï¼Œå½¢ç‹€: {img_array.shape}, æ•¸æ“šé¡å‹: {img_array.dtype} / Loaded numpy array, shape: {img_array.shape}, dtype: {img_array.dtype}")
                
                # ä¿æŒRGBDæ•¸æ“šçš„4å€‹é€šé“ / Keep all 4 channels for RGBD data
                if len(img_array.shape) == 3 and img_array.shape[2] == 4:
                    print("ä½¿ç”¨RGBD 4é€šé“æ•¸æ“š / Using RGBD 4-channel data")
                elif len(img_array.shape) == 3 and img_array.shape[2] == 3:
                    print("ä½¿ç”¨RGB 3é€šé“æ•¸æ“š / Using RGB 3-channel data")
                
                # ç›´æ¥ä½¿ç”¨numpyæ•¸çµ„ä½œç‚ºsource / Use numpy array directly as source
                image_path = img_array
            
            # é€²è¡Œæ¨ç†
            result = model.predict(
                source=image_path,
                device=device,
                project=os.path.dirname(result_dir),
                name=os.path.basename(result_dir),
                exist_ok=True,  # å¦‚æœç›®éŒ„å·²å­˜åœ¨å‰‡ä½¿ç”¨ç¾æœ‰ç›®éŒ„ï¼Œä¸å‰µå»ºæ–°çš„ / Use existing directory if it exists
                save=True,  # ä¿å­˜çµæœåœ–ç‰‡ / Save result images
                save_txt=True,  # ä¿å­˜æ–‡æœ¬çµæœ / Save text results
                save_conf=True,  # ä¿å­˜ç½®ä¿¡åº¦ / Save confidence
                show=False,  # ä¸é¡¯ç¤ºåœ–ç‰‡ / Don't show images
                verbose=False,  # æ¸›å°‘è¼¸å‡º / Reduce output
                conf=confidence_threshold,  # ç½®ä¿¡åº¦é–¾å€¼ / Confidence threshold
                iou=0.45,  # NMS IoUé–¾å€¼ / NMS IoU threshold
                max_det=300,  # æœ€å¤§æª¢æ¸¬æ•¸é‡ / Maximum detections
                line_width=3,  # é‚Šæ¡†ç·šå¯¬ / Bounding box line width
                show_labels=True,  # é¡¯ç¤ºæ¨™ç±¤ / Show labels
                show_conf=True,  # é¡¯ç¤ºç½®ä¿¡åº¦ / Show confidence
                save_crop=False,  # ä¸ä¿å­˜è£å‰ª / Don't save crops
                visualize=False, # å•Ÿç”¨å¯è¦–åŒ– / Enable visualization
                augment=False,  # ä¸ä½¿ç”¨æ•¸æ“šå¢å¼· / Don't use data augmentation
                agnostic_nms=False,  # ä¸ä½¿ç”¨é¡åˆ¥ç„¡é—œNMS / Don't use agnostic NMS
                retina_masks=False,  # ä¸ä½¿ç”¨è¦–ç¶²è†œé®ç½© / Don't use retina masks
                show_boxes=True,  # é¡¯ç¤ºé‚Šæ¡† / Show boxes
                format='torch'  # è¿”å›torchæ ¼å¼ / Return torch format
            )
            
            # å°‡çµæœæ·»åŠ åˆ°åˆ—è¡¨ä¸­
            if result:
                results.extend(result)
            
            print(f"âœ… æˆåŠŸè™•ç† {image_file} / âœ… Successfully processed {image_file}")
            
        except Exception as e:
            print(f"âŒ è™•ç† {image_file} å¤±æ•—: {e} / âŒ Failed to process {image_file}: {e}")
            continue
    
    print("æ¨ç†å®Œæˆ / Inference completed")
    print(f"æˆåŠŸè™•ç† {len(results)} å¼µåœ–ç‰‡ / Successfully processed {len(results)} images")
    print(f"çµæœä¿å­˜åœ¨: {result_dir} / Results saved to: {result_dir}")
    
    return results

if __name__ == '__main__':
    # å¦‚æœç›´æ¥é‹è¡Œï¼Œéœ€è¦æä¾›æ¨¡å‹è·¯å¾‘
    import sys
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python yolo_inference.py <model_path> [confidence_threshold] [device] [predict_data_dir]")
        print("Example: python yolo_inference.py model.pt 0.25 cuda Predict/Data")
        sys.exit(1)
    
    model_path = sys.argv[1]
    confidence_threshold = float(sys.argv[2]) if len(sys.argv) > 2 else 0.25
    device = sys.argv[3] if len(sys.argv) > 3 else None
    predict_data_dir = sys.argv[4] if len(sys.argv) > 4 else None
    
    main(model_path, confidence_threshold, device, predict_data_dir)


def enhanced_inference(model_path, confidence_threshold=0.25, device=None, predict_data_dir=None,
                      iou_threshold=0.45, max_det=300, line_width=3, show_labels=True, 
                      show_conf=True, show_boxes=True, save_txt=True, save_conf=True, 
                      save_crop=False, visualize=True, augment=False, agnostic_nms=False, 
                      retina_masks=False, output_format='torch', verbose=False, show=False):
    """
    å¢å¼·ç‰ˆæ¨ç†å‡½æ•¸ - æ”¯æŒæ‰€æœ‰model.predict()åƒæ•¸
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾‘ (å¿…éœ€)
        confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼ï¼Œé»˜èª0.25
        device: è¨­å‚™é¡å‹ï¼Œå¦‚æœç‚ºNoneå‰‡è‡ªå‹•æª¢æ¸¬
        predict_data_dir: é æ¸¬æ•¸æ“šç›®éŒ„ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨é»˜èªç›®éŒ„
        iou_threshold: IoUé–¾å€¼ï¼Œé»˜èª0.45
        max_det: æœ€å¤§æª¢æ¸¬æ•¸é‡ï¼Œé»˜èª300
        line_width: é‚Šæ¡†ç·šå¯¬ï¼Œé»˜èª3
        show_labels: é¡¯ç¤ºæ¨™ç±¤ï¼Œé»˜èªTrue
        show_conf: é¡¯ç¤ºç½®ä¿¡åº¦ï¼Œé»˜èªTrue
        show_boxes: é¡¯ç¤ºé‚Šæ¡†ï¼Œé»˜èªTrue
        save_txt: ä¿å­˜æ–‡æœ¬çµæœï¼Œé»˜èªTrue
        save_conf: ä¿å­˜ç½®ä¿¡åº¦ï¼Œé»˜èªTrue
        save_crop: ä¿å­˜è£å‰ªï¼Œé»˜èªFalse
        visualize: å•Ÿç”¨å¯è¦–åŒ–ï¼Œé»˜èªTrue
        augment: æ•¸æ“šå¢å¼·ï¼Œé»˜èªFalse
        agnostic_nms: é¡åˆ¥ç„¡é—œNMSï¼Œé»˜èªFalse
        retina_masks: è¦–ç¶²è†œé®ç½©ï¼Œé»˜èªFalse
        output_format: è¼¸å‡ºæ ¼å¼ï¼Œé»˜èª'torch'
        verbose: è©³ç´°è¼¸å‡ºï¼Œé»˜èªFalse
        show: é¡¯ç¤ºåœ–ç‰‡ï¼Œé»˜èªFalse
    """
    # ä¿®å¾© plot_images å‡½æ•¸
    fix_plot_images_function()
    
    # æª¢æŸ¥å¿…éœ€åƒæ•¸
    if not model_path:
        print("éŒ¯èª¤: å¿…é ˆæä¾›æ¨¡å‹æ–‡ä»¶è·¯å¾‘ / Error: Model path is required")
        return
    
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device} / Using device: {device}")

    # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"éŒ¯èª¤: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ / Error: Model file not found: {model_path}")
        return

    # è¼‰å…¥æ¨¡å‹
    print("è¼‰å…¥æ¨¡å‹ä¸­... / Loading model...")
    model = YOLO(model_path)
    print("æ¨¡å‹è¼‰å…¥æˆåŠŸ / Model loaded successfully")

    # è¨­ç½®é æ¸¬æ•¸æ“šç›®éŒ„
    if predict_data_dir is None:
        predict_data_dir = r"Predict\Data"
    
    if not os.path.exists(predict_data_dir):
        print(f"éŒ¯èª¤: é æ¸¬æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨ / Error: Predict data directory not found: {predict_data_dir}")
        return
    
    # æª¢æŸ¥æ˜¯å¦æœ‰åœ–ç‰‡æ–‡ä»¶
    image_files = []
    for ext in ['.npy', '.jpg', '.jpeg', '.png', '.bmp']:
        image_files.extend([f for f in os.listdir(predict_data_dir) if f.lower().endswith(ext)])
    
    if not image_files:
        print(f"éŒ¯èª¤: åœ¨ {predict_data_dir} ä¸­æœªæ‰¾åˆ°åœ–ç‰‡æ–‡ä»¶ / Error: No image files found in {predict_data_dir}")
        return
    
    print(f"æ‰¾åˆ° {len(image_files)} å€‹åœ–ç‰‡æ–‡ä»¶ / Found {len(image_files)} image files")
    
    # è¨­ç½®çµæœç›®éŒ„ - ä½¿ç”¨å¸¶æ™‚é–“æˆ³çš„runsç›®éŒ„
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%dT%H%M")
    result_dir = f"runs/yolo_inference/{timestamp}"
    os.makedirs(result_dir, exist_ok=True)
    
    # å°æ¯å¼µåœ–ç‰‡é€²è¡Œæ¨ç†
    results = []
    for i, image_file in enumerate(image_files):
        try:
            print(f"è™•ç†åœ–ç‰‡ {i+1}/{len(image_files)}: {image_file} / Processing image {i+1}/{len(image_files)}: {image_file}")
            
            # æ§‹å»ºå®Œæ•´è·¯å¾‘
            image_path = os.path.join(predict_data_dir, image_file)
            
            # å¦‚æœæ˜¯.npyæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨numpyæ•¸çµ„
            if image_file.lower().endswith('.npy'):
                img_array = np.load(image_path)
                print(f"è¼‰å…¥numpyæ•¸çµ„ï¼Œå½¢ç‹€: {img_array.shape}, æ•¸æ“šé¡å‹: {img_array.dtype} / Loaded numpy array, shape: {img_array.shape}, dtype: {img_array.dtype}")
                
                if len(img_array.shape) == 3 and img_array.shape[2] == 4:
                    print("ä½¿ç”¨RGBD 4é€šé“æ•¸æ“š / Using RGBD 4-channel data")
                elif len(img_array.shape) == 3 and img_array.shape[2] == 3:
                    print("ä½¿ç”¨RGB 3é€šé“æ•¸æ“š / Using RGB 3-channel data")
                
                image_path = img_array
            
            # é€²è¡Œæ¨ç† - ä½¿ç”¨æ‰€æœ‰é«˜ç´šåƒæ•¸
            result = model.predict(
                source=image_path,
                device=device,
                project=os.path.dirname(result_dir),
                name=os.path.basename(result_dir),
                exist_ok=True,
                save=True,
                save_txt=save_txt,
                save_conf=save_conf,
                show=show,
                verbose=verbose,
                conf=confidence_threshold,
                iou=iou_threshold,
                max_det=max_det,
                line_width=line_width,
                show_labels=show_labels,
                show_conf=show_conf,
                save_crop=save_crop,
                visualize=visualize,
                augment=augment,
                agnostic_nms=agnostic_nms,
                retina_masks=retina_masks,
                show_boxes=show_boxes,
                format=output_format
            )
            
            print(f"âœ… æˆåŠŸè™•ç† {image_file} / âœ… Successfully processed {image_file}")
            results.append(result)
            
        except Exception as e:
            print(f"âŒ è™•ç† {image_file} å¤±æ•—: {e} / âŒ Failed to process {image_file}: {e}")
            continue

    print("æ¨ç†å®Œæˆ / Inference completed")
    print(f"çµæœä¿å­˜åœ¨: {result_dir} / Results saved to: {result_dir}")
    return results